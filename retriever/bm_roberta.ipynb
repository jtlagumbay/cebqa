{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtlagumbay/cebqa/blob/main/retriever/bm_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otq_L0hL6e2P"
      },
      "source": [
        "# **QA Pipeline**\n",
        "\n",
        "1. ElasticSeach Indexer\n",
        "2. BM25 Retriever\n",
        "3. Fine-tuned XLMR Reader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOvUOWAOgjQj"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPzhJswwfHQZ",
        "outputId": "493aa947-fd27-4d2b-b8d8-5fcfa5cc0864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-9.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch)\n",
            "  Downloading elastic_transport-8.17.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from elasticsearch) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from elasticsearch) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->elasticsearch) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Downloading elasticsearch-9.0.0-py3-none-any.whl (895 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.8/895.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading elastic_transport-8.17.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, xxhash, rank_bm25, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, elastic-transport, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, elasticsearch, nvidia-cusolver-cu12, datasets, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 elastic-transport-8.17.1 elasticsearch-9.0.0 evaluate-0.4.3 fsspec-2024.12.0 fuzzywuzzy-0.18.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rank_bm25-0.2.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install elasticsearch transformers datasets evaluate rank_bm25 nltk fuzzywuzzy sentence_transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kOoiI65fzNT"
      },
      "outputs": [],
      "source": [
        "# pip install --upgrade --no-cache-dir numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd5lCHKLfzNT",
        "outputId": "b693d697-ce3f-4e64-e209-3b3adf63f2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2596jflKFZi",
        "outputId": "c500f007-be59-45f1-b3aa-9d2a4f8b67d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch.helpers import bulk\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer, XLMRobertaTokenizerFast, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "from datasets import Dataset, load_dataset\n",
        "import re\n",
        "from evaluate import load\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "from fuzzywuzzy import fuzz\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import faiss\n",
        "import unicodedata\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ppd3LkTdLVbR"
      },
      "outputs": [],
      "source": [
        "CEBQA_DATASET = \"jhoannarica/cebquad_split\"\n",
        "SUPERBALITA_DATASET = \"jhoannarica/superbalita_split\"\n",
        "ELASTIC_URL = \"https://tender-separately-mudfish.ngrok-free.app\"\n",
        "# ELASTIC_URL = \"http://localhost:9200\"\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/My Drive\"\n",
        "# DRIVE_ROOT = \"/Users/jhoannaricalagumbay/Library/CloudStorage/GoogleDrive-jtlagumbay@up.edu.ph/My Drive\"\n",
        "\n",
        "READER_FOLDER = \"/UP Files/IV - 2nd sem/CMSC 198.1/cebqa_roberta/new-split/xlmr_body-filtered/2025-04-04_05-13\"\n",
        "DPR_FOLDER = \"/UP Files/IV - 2nd sem/CMSC 198.1/cebqa_roberta/dpr/2025-04-16_03-37\"\n",
        "\n",
        "\n",
        "CURRENT_MODEL = DRIVE_ROOT + READER_FOLDER + \"/model\"\n",
        "CURRENT_TOKENIZER = DRIVE_ROOT + READER_FOLDER + \"/tokenizer\"\n",
        "INDEX_NAME = \"superbalita\"\n",
        "K = 10\n",
        "BM25 = \"bm25\"\n",
        "FAISS = \"faiss\"\n",
        "\n",
        "CEBQA_DPR_MODEL = DRIVE_ROOT + DPR_FOLDER + \"/model\"\n",
        "CEBQA_DPR_TOKENIZER = DRIVE_ROOT + DPR_FOLDER + \"/tokenizer\"\n",
        "DPR_CONTEXT_ENCODER = \"voidful/dpr-ctx_encoder-bert-base-multilingual\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8h9VpP0gnfK"
      },
      "source": [
        "# Indexer\n",
        "\n",
        "Start ElasticSearch Locally:\n",
        "1. Start ES docker\n",
        "2. Start NGROK: `ngrok http --url=tender-separately-mudfish.ngrok-free.app 9200`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXMwFDbHvpi7",
        "outputId": "788b35a1-3b7f-4b48-9dbd-4cad63de74d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Allow': 'GET,DELETE,HEAD', 'Content-Length': '0', 'Content-Type': 'text/plain; charset=UTF-8', 'Ngrok-Agent-Ips': '124.217.18.14', 'X-Elastic-Product': 'Elasticsearch', 'Date': 'Thu, 17 Apr 2025 07:27:45 GMT'}\n"
          ]
        }
      ],
      "source": [
        "headers = {\n",
        "    \"Origin\": \"https://colab.research.google.com\",\n",
        "     \"Content-Type\": \"application/json\",\n",
        "    \"Accept\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.options(ELASTIC_URL, headers=headers)\n",
        "print(response.headers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "tCzdjxWMGbQI",
        "outputId": "bc80d0b6-bcd6-4761-e084-9c2fb04283fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/__init__.py:311: SecurityWarning: Connecting to 'https://tender-separately-mudfish.ngrok-free.app:443' using TLS with verify_certs=False is insecure\n",
            "  _transport = transport_class(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'tender-separately-mudfish.ngrok-free.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Accept, Content-Type]')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-fe89057b85d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElasticsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mELASTIC_URL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_certs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     print(es.transport.perform_request('GET', '/'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# except Exception as e:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/__init__.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, error_trace, filter_path, human, pretty)\u001b[0m\n\u001b[1;32m   2989\u001b[0m             \u001b[0m__query\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pretty\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2990\u001b[0m         \u001b[0m__headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"accept\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2991\u001b[0;31m         return self.perform_request(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   2992\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0m__path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mpath_parts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_parts\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         ) as otel_span:\n\u001b[0;32m--> 271\u001b[0;31m             response = self._perform_request(\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             raise HTTP_EXCEPTIONS.get(meta.status, ApiError)(\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresp_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             )\n",
            "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Accept, Content-Type]')"
          ]
        }
      ],
      "source": [
        "es = Elasticsearch([ELASTIC_URL], verify_certs=False, headers=headers, request_timeout=30)\n",
        "print(es.info())\n",
        "# try:\n",
        "#     print(es.transport.perform_request('GET', '/'))\n",
        "# except Exception as e:\n",
        "#     print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IACpNh6CgpD7"
      },
      "outputs": [],
      "source": [
        "class ElasticSearchIndexer:\n",
        "    def __init__(self, index_name=INDEX_NAME):\n",
        "        self.index_name = index_name\n",
        "        self.es = Elasticsearch(ELASTIC_URL)  # Ensure ES is running\n",
        "        print(f\"Initiating ESIndexer {self.index_name}\")\n",
        "\n",
        "    def create_index(self):\n",
        "        \"\"\" Create an index with a text field for BM25 \"\"\"\n",
        "        if not self.es.indices.exists(index=self.index_name):\n",
        "            self.es.indices.create(index=self.index_name, body={\n",
        "                \"settings\": {\n",
        "                    \"number_of_shards\": 1,\n",
        "                    \"number_of_replicas\": 0\n",
        "                },\n",
        "                \"mappings\": {\n",
        "                    \"properties\": {\n",
        "                        \"id\": {\"type\": \"keyword\"},\n",
        "                        \"title\": {\"type\": \"text\"},\n",
        "                        \"body\": {\"type\": \"text\"}\n",
        "                    }\n",
        "                }\n",
        "            })\n",
        "            print(f\"Index '{self.index_name}' created.\")\n",
        "\n",
        "    def index_documents(self, documents):\n",
        "        \"\"\" Bulk index documents into ElasticSearch \"\"\"\n",
        "        actions = [\n",
        "            {\n",
        "                \"_index\": self.index_name,\n",
        "                \"_id\": doc[\"id\"],  # Use document ID for uniqueness\n",
        "                \"_source\": {\n",
        "                    \"id\": doc[\"id\"],\n",
        "                    \"title\": doc[\"pseudonymized_title\"],\n",
        "                    \"body\": doc[\"pseudonymized_body\"]\n",
        "                }\n",
        "            }\n",
        "            for doc in documents\n",
        "        ]\n",
        "        bulk(self.es, actions)\n",
        "        print(f\"Indexed {len(documents)} documents.\")\n",
        "\n",
        "    def index_from_csv(self, file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        documents = df.to_dict(orient=\"records\")  # Convert DataFrame to a list of dicts\n",
        "        self.index_documents(documents)\n",
        "\n",
        "    def index_from_huggingface(self, dataset = SUPERBALITA_DATASET):\n",
        "        dataset_obj = load_dataset(dataset)\n",
        "\n",
        "        # Create the dataset_dict from the loaded dataset object's splits\n",
        "        dataset_dict = {\n",
        "            split_name: split_dataset\n",
        "            for split_name, split_dataset in dataset_obj.items()\n",
        "        }\n",
        "\n",
        "        all_documents = []\n",
        "        for split_name, split_dataset in dataset_dict.items():\n",
        "            documents = split_dataset.to_list()\n",
        "            all_documents.extend(documents)\n",
        "\n",
        "        self.index_documents(all_documents)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNaD93tOiGnj"
      },
      "outputs": [],
      "source": [
        "# # Sample usage\n",
        "# indexer = ElasticSearchIndexer()\n",
        "# indexer.create_index()\n",
        "# indexer.index_from_csv(\"/Users/jhoannaricalagumbay/School/cebqa/dataset/articles_202503120405_author_removed_fixed.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjVIRDtt2Kjh"
      },
      "source": [
        "# BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VWU0iGhM2Mr5"
      },
      "outputs": [],
      "source": [
        "class BM25Retriever:\n",
        "    def __init__(self, index_name = INDEX_NAME):\n",
        "        print(f\"Initiating retriever with index_name: {INDEX_NAME}\")\n",
        "        self.indexer = ElasticSearchIndexer(index_name = INDEX_NAME)\n",
        "        self.indexer.create_index()\n",
        "        self.indexer.index_from_huggingface()\n",
        "\n",
        "    def retrieve(self, query, top_k=3):\n",
        "        \"\"\" Retrieve top-k relevant documents using BM25 \"\"\"\n",
        "        print(f\"retrieving {top_k} docs for [{query}]\")\n",
        "        response = self.indexer.es.search(index=self.indexer.index_name, body={\n",
        "            \"query\": {\n",
        "                \"match\": {\n",
        "                    \"body\": query\n",
        "                }\n",
        "            },\n",
        "            \"size\": top_k\n",
        "        })\n",
        "        return [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
        "\n",
        "    def retrieve_batch(self, queries, top_k=3):\n",
        "        print(f\"Retrieve Batch for {len(queries)} queries\")\n",
        "        \"\"\" Retrieve top-k relevant documents for multiple queries using BM25 in batch mode \"\"\"\n",
        "        if not isinstance(queries, list):\n",
        "            raise ValueError(\"queries should be a list of strings\")\n",
        "\n",
        "        # Multi-search request body\n",
        "        request_body = \"\"\n",
        "        for query in queries:\n",
        "            safe_question = json.dumps(query)\n",
        "            request_body += f'{{\"index\": \"{self.indexer.index_name}\"}}\\n'  # Metadata\n",
        "            request_body += f'{{\"query\": {{\"match\": {{\"body\": {safe_question}}}}}, \"size\": {top_k}}}\\n'  # Query\n",
        "\n",
        "        # Send multi-search request\n",
        "        response = self.indexer.es.msearch(body=request_body)\n",
        "\n",
        "        # Extract results\n",
        "        results = []\n",
        "        for query_response in response[\"responses\"]:\n",
        "            retrieved_docs = [hit[\"_source\"] for hit in query_response[\"hits\"][\"hits\"]]\n",
        "            results.append(retrieved_docs)\n",
        "\n",
        "        return results  # List of lists, where each sublist contains retrieved documents for a query\n",
        "\n",
        "    def retrieve_batch_query_dict(self, queries_list, top_k=3):\n",
        "        print(f\"Retrieve Batch Dict for {len(queries_list)} queries\")\n",
        "\n",
        "        \"\"\" Retrieve top-k relevant documents for multiple queries using BM25 in batch mode.\n",
        "\n",
        "        Args:\n",
        "            queries_list (list): A list of dictionaries, each containing 'id' and 'question'.\n",
        "            top_k (int): Number of top relevant documents to retrieve per query.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary where keys are query IDs and values are lists of retrieved documents.\n",
        "        \"\"\"\n",
        "        if not isinstance(queries_list, list) or not all(isinstance(q, dict) and 'id' in q and 'question' in q for q in queries_list):\n",
        "            raise ValueError(\"queries_list should be a list of dictionaries with 'id' and 'question' keys\")\n",
        "\n",
        "        # Multi-search request body\n",
        "        request_body = \"\"\n",
        "        query_ids = []  # To track IDs in order\n",
        "        for query in queries_list:\n",
        "            safe_question = json.dumps(query[\"question\"])\n",
        "            query_ids.append(query[\"id\"])\n",
        "            request_body += f'{{\"index\": \"{self.index_name}\"}}\\n'  # Metadata\n",
        "            request_body += f'{{\"query\": {{\"match\": {{\"body\": {safe_question}}}}}, \"size\": {top_k}}}\\n'  # Query\n",
        "\n",
        "        # Send multi-search request\n",
        "        response = self.indexer.es.msearch(body=request_body)\n",
        "\n",
        "        # Extract results and associate with query IDs\n",
        "        results = []\n",
        "        for i, query_response in enumerate(response[\"responses\"]):\n",
        "            retrieved_docs = [hit[\"_source\"] for hit in query_response[\"hits\"][\"hits\"]]\n",
        "            results.append({\n",
        "                \"query_id\": str(query_ids[i]),\n",
        "                \"top_docs\": retrieved_docs\n",
        "            })\n",
        "\n",
        "        return results  # Dictionary format: {id: retrieved_docs}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "hsSSqnQt2rxX",
        "outputId": "c006da95-3a72-450e-a1e0-da9c4cb16242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating retriever with index_name: superbalita\n",
            "Initiating ESIndexer superbalita\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "BadRequestError(400, 'None')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c0f3a6e2dc33>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sample usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM25Retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Unsa ang giingon ni Gobernador Abalayan nga mabuhat ra \"with a united country\"?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtop_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retrieved Documents:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8b88a4896ef7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, index_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initiating retriever with index_name: {INDEX_NAME}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElasticSearchIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINDEX_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_from_huggingface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-141d53ea37d4>\u001b[0m in \u001b[0;36mcreate_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"\"\" Create an index with a text field for BM25 \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             self.es.indices.create(index=self.index_name, body={\n\u001b[1;32m     11\u001b[0m                 \"settings\": {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/indices.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, index, allow_no_indices, error_trace, expand_wildcards, filter_path, flat_settings, human, ignore_unavailable, include_defaults, local, pretty)\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0m__query\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pretty\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0m__headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"accept\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1523\u001b[0;31m         return self.perform_request(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   1524\u001b[0m             \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0m__path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# Use the internal clients .perform_request() implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# so we take advantage of their transport options.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         return self._client.perform_request(\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mpath_parts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_parts\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         ) as otel_span:\n\u001b[0;32m--> 271\u001b[0;31m             response = self._perform_request(\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             raise HTTP_EXCEPTIONS.get(meta.status, ApiError)(\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresp_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             )\n",
            "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'None')"
          ]
        }
      ],
      "source": [
        "# Sample usage\n",
        "retriever = BM25Retriever()\n",
        "query = ['Unsa ang giingon ni Gobernador Abalayan nga mabuhat ra \"with a united country\"?']\n",
        "top_docs = retriever.retrieve_batch(query)\n",
        "print(\"Retrieved Documents:\", top_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACSdoCoTfzNV"
      },
      "source": [
        "# FAISS Indexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "emJXIlwMfzNV"
      },
      "outputs": [],
      "source": [
        "class FAISSIndexer:\n",
        "    def __init__(self, index_file=\"faiss_index.idx\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\", use_fine_tuned = False):\n",
        "        self.index_file = index_file\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        print(\"Loading DPR multilingual context encoder...\")\n",
        "        self.dpr_tokenizer = DPRContextEncoderTokenizer.from_pretrained(DPR_CONTEXT_ENCODER)\n",
        "        self.dpr_model = DPRContextEncoder.from_pretrained(DPR_CONTEXT_ENCODER)\n",
        "        self.use_fine_tuned = use_fine_tuned\n",
        "        self.index = None\n",
        "        self.documents = []  # Store original text\n",
        "        # self.index_from_csv()\n",
        "        self.index_from_huggingface()\n",
        "        print(\"FAISS Indexer initialized.\")\n",
        "\n",
        "\n",
        "    def encode_contexts_with_dpr(self):\n",
        "        \"\"\"Encode contexts using DPR multilingual context encoder and re-index.\"\"\"\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dpr_model.to(device)\n",
        "        self.dpr_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = self.dpr_tokenizer(\n",
        "                self.documents,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512\n",
        "            ).to(device)\n",
        "\n",
        "            embeddings = self.dpr_model(**inputs).pooler_output.cpu().numpy()\n",
        "\n",
        "            if not embeddings.flags['C_CONTIGUOUS']:\n",
        "                embeddings = np.ascontiguousarray(embeddings)\n",
        "\n",
        "            faiss.normalize_L2(embeddings)\n",
        "            return embeddings\n",
        "\n",
        "    def create_index(self, d):\n",
        "        \"\"\"Create a new FAISS index.\"\"\"\n",
        "        self.index = faiss.IndexFlatL2(d)\n",
        "        print(f\"Created FAISS index with dimension {d}.\")\n",
        "\n",
        "    def index_documents(self, documents):\n",
        "        \"\"\"Index documents into FAISS.\"\"\"\n",
        "        self.documents = [doc['pseudonymized_body'] for doc in documents]\n",
        "        self.article_ids = [doc['id'] for doc in documents]\n",
        "        self.titles = [doc['pseudonymized_title'] for doc in documents]\n",
        "\n",
        "        if self.use_fine_tuned:\n",
        "            embeddings = self.encode_contexts_with_dpr()\n",
        "        else:\n",
        "            embeddings = self.model.encode(self.documents, convert_to_numpy=True)\n",
        "\n",
        "        d = embeddings.shape[1]\n",
        "\n",
        "        if self.index is None:\n",
        "            self.create_index(d)\n",
        "\n",
        "        self.index.add(embeddings)\n",
        "        print(f\"Indexed {len(documents)} documents into FAISS.\")\n",
        "        self.save_index()\n",
        "\n",
        "    def index_from_csv(self, file_path):\n",
        "        \"\"\"Load documents from a CSV file and index them.\"\"\"\n",
        "        df = pd.read_csv(file_path)\n",
        "        documents = df.to_dict(orient=\"records\")\n",
        "        self.index_documents(documents)\n",
        "\n",
        "    def index_from_huggingface(self, dataset = SUPERBALITA_DATASET):\n",
        "        dataset_obj = load_dataset(dataset)\n",
        "\n",
        "        # Create the dataset_dict from the loaded dataset object's splits\n",
        "        dataset_dict = {\n",
        "            split_name: split_dataset\n",
        "            for split_name, split_dataset in dataset_obj.items()\n",
        "        }\n",
        "        all_documents = []\n",
        "        for split_name, split_dataset in dataset_dict.items():\n",
        "            documents = split_dataset.to_list()\n",
        "            all_documents.extend(documents)\n",
        "\n",
        "        self.index_documents(all_documents)\n",
        "\n",
        "    def save_index(self):\n",
        "        \"\"\"Save FAISS index to disk.\"\"\"\n",
        "        faiss.write_index(self.index, self.index_file)\n",
        "        print(f\"FAISS index saved to {self.index_file}.\")\n",
        "\n",
        "    def load_index(self):\n",
        "        \"\"\"Load FAISS index from disk.\"\"\"\n",
        "        self.index = faiss.read_index(self.index_file)\n",
        "        print(f\"FAISS index loaded from {self.index_file}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "T0jb0opFfzNV"
      },
      "outputs": [],
      "source": [
        "class FAISSRetriever:\n",
        "    def __init__(\n",
        "            self,\n",
        "            q_encoder = DPRQuestionEncoder.from_pretrained(CEBQA_DPR_MODEL),\n",
        "            q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(CEBQA_DPR_TOKENIZER),\n",
        "            index_file=\"faiss_index.idx\",\n",
        "            top_k=3,\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            use_fine_tuned = False\n",
        "        ):\n",
        "        print(\"Initializing FAISS Retriever\")\n",
        "        self.indexer = FAISSIndexer(index_file=index_file, use_fine_tuned=use_fine_tuned)\n",
        "        self.top_k = top_k\n",
        "        self.use_fine_tuned = use_fine_tuned\n",
        "        self.q_encoder = q_encoder\n",
        "        self.q_tokenizer = q_tokenizer\n",
        "        self.device = device\n",
        "\n",
        "        # Move model to the appropriate device\n",
        "        self.q_encoder.to(device)\n",
        "        self.q_encoder.eval()\n",
        "\n",
        "    def encode_query(self, query):\n",
        "        \"\"\"Encode the query using the fine-tuned question encoder.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            inputs = self.q_tokenizer(\n",
        "                query,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=64\n",
        "            ).to(self.device)\n",
        "\n",
        "            embeddings = self.q_encoder(**inputs).pooler_output.cpu().numpy()\n",
        "\n",
        "            # Ensure embeddings are C-contiguous for FAISS\n",
        "            if not embeddings.flags['C_CONTIGUOUS']:\n",
        "                embeddings = np.ascontiguousarray(embeddings)\n",
        "\n",
        "            # Normalize embeddings for cosine similarity\n",
        "            faiss.normalize_L2(embeddings)\n",
        "\n",
        "            return embeddings\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        \"\"\"Retrieve top-k relevant documents using FAISS.\"\"\"\n",
        "        if self.use_fine_tuned:\n",
        "            query_embedding = self.encode_query([query])\n",
        "        else:\n",
        "            query_embedding = self.indexer.model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "        D, I = self.indexer.index.search(query_embedding, self.top_k)\n",
        "        print(D, I)\n",
        "        results = [\n",
        "            {\n",
        "                \"rank\": rank + 1,\n",
        "                \"score\": float(D[0][rank]),\n",
        "                \"id\": self.indexer.article_ids[idx],\n",
        "                \"title\": self.indexer.titles[idx],\n",
        "                \"body\": self.indexer.documents[idx],\n",
        "            }\n",
        "            for rank, idx in enumerate(I[0]) if idx < len(self.indexer.documents)\n",
        "        ]\n",
        "        return results\n",
        "\n",
        "    def retrieve_batch(self, queries):\n",
        "\n",
        "        print(f\"processing {len(queries)}\")\n",
        "        \"\"\"Retrieve top-k relevant documents for multiple queries.\"\"\"\n",
        "        questions = [query[\"question\"] for query in queries]\n",
        "        if self.use_fine_tuned:\n",
        "            query_embeddings = self.encode_query([query])\n",
        "        else:\n",
        "            query_embeddings = self.indexer.model.encode(questions, convert_to_numpy=True)\n",
        "        D, I = self.indexer.index.search(query_embeddings, self.top_k)\n",
        "        print(f\"done {len(D)}\")\n",
        "        results = []\n",
        "        for query_idx, query in enumerate(queries):\n",
        "            print(f\"query idx: {query_idx}\")\n",
        "            retrieved_docs = [\n",
        "                {\n",
        "                    \"rank\": rank + 1,\n",
        "                    \"score\": float(D[query_idx][rank]),\n",
        "                    \"text\": self.indexer.documents[idx]\n",
        "                }\n",
        "                for rank, idx in enumerate(I[query_idx]) if idx < len(self.indexer.documents)\n",
        "            ]\n",
        "            results.append({\"query\": query, \"top_docs\": retrieved_docs})\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhQrz7eDfzNV"
      },
      "outputs": [],
      "source": [
        "# # Initialize the FAISS indexer\n",
        "# indexer = FAISSIndexer(index_file=\"faiss_index.idx\")\n",
        "\n",
        "# Index documents from a CSV file\n",
        "# # Save the FAISS index for later use\n",
        "# indexer.save_index()\n",
        "\n",
        "# indexer = FAISSIndexer(index_file=\"faiss_index.idx\")\n",
        "# indexer.load_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnZAxJ11fzNW"
      },
      "outputs": [],
      "source": [
        "# Initialize the retriever with the loaded indexer\n",
        "# retriever = FAISSRetriever(index_file=\"faiss_index.idx\", top_k=K)\n",
        "\n",
        "# # Retrieve relevant documents for a single query\n",
        "# query = \"kanus-a ang palarong pambansa??\"\n",
        "# results = retriever.retrieve(query)\n",
        "\n",
        "# # Print retrieved documents\n",
        "# print(results)\n",
        "\n",
        "\n",
        "# Retrieve relevant documents for a single query\n",
        "# query = [{\"question\": \"kanus-a ang palarong pambansa?\"}, {\"question\":\"Kinsa ang hepe sa Cebu Police?\"}]\n",
        "# results = retriever.retrieve_batch(query)\n",
        "\n",
        "# # Print retrieved documents\n",
        "# for res in results:\n",
        "#     print(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwo3IpR3--n"
      },
      "source": [
        "# Reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZAb9aRYL4AYY"
      },
      "outputs": [],
      "source": [
        "class Reader:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path = CURRENT_MODEL,\n",
        "        tokenizer_path = CURRENT_TOKENIZER\n",
        "      ):\n",
        "        print(f\"Initiating reader with model: {model_path}\")\n",
        "        model_best = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "        tokenizer_best = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "        # device = torch.device(\"mps\")\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.qa_pipeline = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=model_best,\n",
        "            tokenizer=tokenizer_best,\n",
        "            device=device\n",
        "            )\n",
        "\n",
        "    def extract_answer_batch(self, queries_list, top_docs):\n",
        "        print(f\"Extracting batch answer for {len(queries_list)} queries\")\n",
        "        qa_dataset = Dataset.from_dict({\n",
        "          \"question\": [queries_list[\"question\"] for doc in top_docs['top_docs']] ,\n",
        "          \"context\": [doc['body'] for doc in top_docs['top_docs']]\n",
        "        })\n",
        "\n",
        "        return self.qa_pipeline(qa_dataset)\n",
        "\n",
        "    def extract_answer(self, question, documents, num_chunks = 1, overlap = 0.3):\n",
        "        print(f\"extracting answer for {question}\")\n",
        "        \"\"\" Find the best answer from retrieved documents while keeping metadata \"\"\"\n",
        "        best_result = None\n",
        "        best_score = 0\n",
        "\n",
        "        for doc in documents:\n",
        "            if num_chunks == 1:\n",
        "                contexts = [doc[\"body\"]]\n",
        "            else:\n",
        "                contexts = self.chunk_text(doc[\"body\"],  num_chunks, overlap)\n",
        "\n",
        "            for context in contexts:\n",
        "            #   print(question)\n",
        "            #   print(context)\n",
        "              result = self.qa_pipeline(question=question, context = context)\n",
        "              if result[\"score\"] > best_score:\n",
        "                  best_result = {\n",
        "                      \"article_id\": doc[\"id\"],\n",
        "                      \"title\": doc[\"title\"],\n",
        "                      \"body\": doc[\"body\"],\n",
        "                      \"answer\": result[\"answer\"],\n",
        "                      \"score\": result[\"score\"]\n",
        "                  }\n",
        "                  best_score = result[\"score\"]\n",
        "\n",
        "        return best_result\n",
        "\n",
        "    def chunk_text(self, text, chunk_size=3, overlap=0.5):\n",
        "        sentences = sent_tokenize(text)  # Tokenize text into sentences\n",
        "        step = int(chunk_size * (1 - overlap))  # Overlapping step\n",
        "\n",
        "        chunks = []\n",
        "        for i in range(0, len(sentences), step):\n",
        "            chunk = sentences[i:i + chunk_size]\n",
        "            if not chunk:\n",
        "                continue\n",
        "            chunks.append(\" \".join(chunk))\n",
        "\n",
        "        return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8burd_D9KK_E"
      },
      "source": [
        "# QA Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "DdkmoxgOi9vo"
      },
      "outputs": [],
      "source": [
        "class QA:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path = CURRENT_MODEL,\n",
        "        tokenizer_path = CURRENT_TOKENIZER,\n",
        "        dataset = CEBQA_DATASET,\n",
        "        indexer_type = BM25,\n",
        "        index_name = INDEX_NAME,\n",
        "        k = K,\n",
        "        sample = None,\n",
        "        isRandom = False,\n",
        "        overlap = 0.0,\n",
        "        num_chunks = 1,\n",
        "        use_fine_tuned = False\n",
        "      ):\n",
        "        reader = Reader(model_path=model_path, tokenizer_path=tokenizer_path)\n",
        "\n",
        "        self.model_path = model_path\n",
        "        self.tokenizer_path = tokenizer_path\n",
        "        self.reader = reader\n",
        "        self.tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
        "        test_dataset = load_dataset(dataset)[\"test\"]\n",
        "        self.dataset = test_dataset.filter(self.filter_incomplete_examples) \\\n",
        "            .map(self.normalize_row, batched=True) \\\n",
        "            .map(self.tokenize_train_function, batched=True)\\\n",
        "            .filter(self.decode_error)\n",
        "        self.sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        self.k = k\n",
        "        self.overlap = overlap\n",
        "        self.num_chunks = num_chunks\n",
        "        self.sample = sample\n",
        "        self.isRandom = isRandom\n",
        "        self.index_name = index_name\n",
        "        self.indexer_type = indexer_type\n",
        "\n",
        "        if sample is not None and isRandom:\n",
        "            indices = random.sample(range(len(self.dataset)), sample)\n",
        "            self.dataset = self.dataset.select(indices)\n",
        "        elif sample is not None and not isRandom:\n",
        "            self.dataset = self.dataset.select(range(sample))\n",
        "\n",
        "        print(f\"Initiating QA Pipeline.\")\n",
        "        print(f\"QA model {self.model_path}\")\n",
        "        print(f\"QA tokenizer {self.tokenizer_path}\")\n",
        "        print(f\"QA reader {self.reader}\")\n",
        "        print(f\"QA dataset {len(self.dataset)}\")\n",
        "        print(f\"QA k {self.k}\")\n",
        "        print(f\"QA overlap {self.overlap}\")\n",
        "        print(f\"QA num_chunks {self.num_chunks}\")\n",
        "        print(f\"QA sample {self.sample}\")\n",
        "        print(f\"QA isRandom {self.isRandom}\")\n",
        "        print(f\"QA index_name {self.index_name}\")\n",
        "        print(f\"QA indexer {self.index_name}\")\n",
        "        self.queries = [\n",
        "            {\n",
        "                \"id\": item['id'],\n",
        "                \"article_id\": item['article_id'],\n",
        "                \"question\": item['question'],\n",
        "                \"context\": {\n",
        "                    \"text\": item['context'],\n",
        "                    \"start\": item['context_start']\n",
        "                },\n",
        "                \"answer\": {\n",
        "                    \"text\": item['answer'],\n",
        "                    \"start\": item['answer_start']\n",
        "                }\n",
        "            }\n",
        "            for item in self.dataset\n",
        "        ]\n",
        "\n",
        "        if indexer_type == BM25:\n",
        "            self.retriever = BM25Retriever(index_name=index_name)\n",
        "            self.run_top_docs_batch_bm25()\n",
        "        else:\n",
        "            self.retriever = FAISSRetriever(top_k=self.k, use_fine_tuned = use_fine_tuned)\n",
        "            self.run_top_docs_batch_faiss()\n",
        "        print(f\"QA retriever {self.retriever}\")\n",
        "\n",
        "\n",
        "    def run_top_docs_batch_bm25(self):\n",
        "        self.top_docs = self.retriever.retrieve_batch_query_dict(\n",
        "            queries_list = self.queries,\n",
        "            top_k=self.k\n",
        "        )\n",
        "\n",
        "        return self.top_docs\n",
        "\n",
        "    def run_top_docs_batch_faiss(self):\n",
        "        docs = []\n",
        "        for item in self.dataset:\n",
        "            result = self.retriever.retrieve(item[\"question\"])\n",
        "            doc = {\n",
        "                \"query_id\": item[\"id\"],\n",
        "                \"top_docs\": result\n",
        "            }\n",
        "            docs.append(doc)\n",
        "\n",
        "        self.top_docs = docs\n",
        "        return self.top_docs\n",
        "\n",
        "    def run(self):\n",
        "        start_time = time.time()\n",
        "        date_now = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))\n",
        "        print(f\"QA run for {self.model_path} on {date_now}\")\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for index, query in enumerate(self.queries):\n",
        "          print(f\"{index} processing {query['id']}\")\n",
        "          docs = self.top_docs[index]['top_docs']\n",
        "        #   print([f\"{doc}\\n\" for doc in docs])\n",
        "          answer = self.reader.extract_answer(\n",
        "              question = query[\"question\"],\n",
        "              documents = docs,\n",
        "              num_chunks = self.num_chunks,\n",
        "              overlap= self.overlap\n",
        "          )\n",
        "        #   print(query[\"answer\"][\"text\"])\n",
        "        #   print(answer[\"answer\"])\n",
        "          result = query\n",
        "          result[\"pred\"] = answer\n",
        "          result[\"top_docs\"] = docs\n",
        "          results.append(result)\n",
        "\n",
        "        self.results = results\n",
        "\n",
        "        end_time = time.time()\n",
        "        self.stats ={\n",
        "            'run_time': end_time - start_time\n",
        "        }\n",
        "        return self.results\n",
        "\n",
        "    def normalize_row(self, examples):\n",
        "        examples[\"context\"] = [unicodedata.normalize(\"NFKC\", context) for context in examples[\"context\"]]\n",
        "\n",
        "        examples[\"article_body\"] = [unicodedata.normalize(\"NFKC\", body) for body in examples[\"article_body\"]]\n",
        "\n",
        "        examples[\"answer\"] =  [unicodedata.normalize(\"NFKC\", answer) for answer in examples[\"answer\"]]\n",
        "\n",
        "        examples[\"question\"] = [unicodedata.normalize(\"NFKC\", q) for q in examples[\"question\"]]\n",
        "\n",
        "        return examples\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        \"\"\"Lowercase and remove punctuation, articles, and extra whitespace.\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\W+', ' ', text)  # Remove punctuation and special characters\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
        "        return text\n",
        "\n",
        "    def compute_similarity(self, text1, text2):\n",
        "        \"\"\"Compute cosine similarity between two texts using Sentence Transformers.\"\"\"\n",
        "        emb1 = self.sentence_transformer.encode(text1, convert_to_tensor=True)\n",
        "        emb2 = self.sentence_transformer.encode(text2, convert_to_tensor=True)\n",
        "        similarity = util.pytorch_cos_sim(emb1, emb2).item()  # Convert tensor to float\n",
        "        return similarity\n",
        "\n",
        "    def evaluate_batch(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate_retriever(self):\n",
        "        wrong_doc = []\n",
        "        for index, query in enumerate(self.queries):\n",
        "            top_doc = self.top_docs[index][\"top_docs\"]\n",
        "            if not any(doc[\"id\"] == query[\"article_id\"] for doc in top_doc):\n",
        "                wrong_doc.append(query[\"article_id\"])\n",
        "\n",
        "        return wrong_doc\n",
        "\n",
        "    def compute_retrieval_metrics(self):\n",
        "        metrics = {\n",
        "            \"hits@1\": 0,\n",
        "            \"hits@3\": 0,\n",
        "            \"hits@5\": 0,\n",
        "            \"hits@10\": 0,\n",
        "            \"mrr\": 0.0,\n",
        "            \"map\": 0.0\n",
        "        }\n",
        "\n",
        "        total = len(self.queries)\n",
        "\n",
        "        for index, query in enumerate(self.queries):\n",
        "            correct_id = query[\"article_id\"]\n",
        "            docs = self.top_docs[index][\"top_docs\"]  # Ranked list of dicts with 'id'\n",
        "\n",
        "            found = False\n",
        "            ap = 0.0  # Average Precision for this query\n",
        "            relevant_count = 0\n",
        "\n",
        "            for rank, doc in enumerate(docs):\n",
        "                                    # rank is 0-based, so add 1\n",
        "                print(rank, doc)\n",
        "\n",
        "                r = rank + 1\n",
        "                if doc[\"id\"] == correct_id:\n",
        "                    relevant_count += 1\n",
        "                    ap += relevant_count / r\n",
        "                    if r <= 1: metrics[\"hits@1\"] += 1\n",
        "                    if r <= 3: metrics[\"hits@3\"] += 1\n",
        "                    if r <= 5: metrics[\"hits@5\"] += 1\n",
        "                    if r <= 10: metrics[\"hits@10\"] += 1\n",
        "                    metrics[\"mrr\"] += 1 / r\n",
        "                    found = True\n",
        "                    break  # stop checking once found\n",
        "\n",
        "            if found:\n",
        "                metrics[\"map\"] += ap  # AP is already calculated in the loop\n",
        "            else:\n",
        "                metrics[\"mrr\"] += 0.0  # optional, for clarity\n",
        "                metrics[\"map\"] += 0.0\n",
        "\n",
        "        # Average over total queries\n",
        "        for k in [\"hits@1\", \"hits@3\", \"hits@5\", \"hits@10\"]:\n",
        "            metrics[k] = metrics[k] / total\n",
        "        metrics[\"mrr\"] = metrics[\"mrr\"] / total\n",
        "        metrics[\"map\"] = metrics[\"map\"] / total\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate(self):\n",
        "        print(f\"QA evaluate for {len(self.results)} results on {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\")\n",
        "        pred = [\n",
        "          {\n",
        "              'id': result['id'],  # Convert ID to string\n",
        "              'prediction_text': self.normalize_text(result['pred']['answer'])\n",
        "          }\n",
        "          for result in self.results\n",
        "        ]\n",
        "\n",
        "        ref = [\n",
        "            {\n",
        "                'id': item['id'],  # Convert ID to string\n",
        "                'answers': {\n",
        "                    'text': [self.normalize_text(item['answer']['text'])],\n",
        "                    'answer_start': [item['answer']['start']]\n",
        "                }\n",
        "            }\n",
        "            for item in self.results\n",
        "        ]\n",
        "\n",
        "        # Load SQuAD metric\n",
        "        metric = load(\"squad\")\n",
        "\n",
        "        # Compute metric\n",
        "        res = metric.compute(predictions=pred, references=ref)\n",
        "        sentence_match_scores = [\n",
        "            p['prediction_text'] in r['answers']['text'][0] for p, r in zip(pred, ref)\n",
        "        ]\n",
        "\n",
        "        # Compute average sentence match score\n",
        "        avg_sentence_match = np.mean(sentence_match_scores)\n",
        "\n",
        "        # Combine results\n",
        "        res[\"sentence_match\"] = float(avg_sentence_match ) * 100\n",
        "        print(res)\n",
        "\n",
        "        self.config = {\n",
        "            'model_path': self.model_path,\n",
        "            'tokenizer_path': self.tokenizer_path,\n",
        "            'k': self.k,\n",
        "            'sample': self.sample,\n",
        "            'isRandom': self.isRandom,\n",
        "            'overlap': self.overlap,\n",
        "            'num_chunks': self.num_chunks,\n",
        "            'indexer_type': self.indexer_type\n",
        "        }\n",
        "        self.eval_res = res\n",
        "\n",
        "        return self.eval_res, self.config, self.stats\n",
        "\n",
        "    def filter_incomplete_examples(self, example):\n",
        "        # Ensure both \"question\" and \"context\" exist and are non-empty\n",
        "        return \"question\" in example and example[\"question\"] and \\\n",
        "            \"article_body\" in example and example[\"answer\"]\n",
        "\n",
        "    def filter_by_token_length(self, example):\n",
        "        # Tokenize the concatenated question + article_body\n",
        "        tokens = self.tokenizer(example[\"question\"], example[\"article_body\"], truncation=False)\n",
        "        return len(tokens[\"input_ids\"]) <= 512\n",
        "\n",
        "    def decode_error(self, example):\n",
        "        input_ids = example[\"input_ids\"]\n",
        "        start_positions = example[\"start_positions\"]\n",
        "        end_positions = example[\"end_positions\"]\n",
        "        predict_answer_tokens = input_ids[start_positions : end_positions+1]\n",
        "        return self.tokenizer.decode(predict_answer_tokens) == example[\"answer\"]\n",
        "\n",
        "    def tokenize_train_function(self, examples):\n",
        "        article_text = [article for article in examples.get(\"article_body\", [\"\"])]\n",
        "        context_text = [context for context in examples.get(\"context\", [\"{}\"])]\n",
        "        answer_text = examples.get(\"answer\", [\"\"])\n",
        "        answer_start = examples.get(\"answer_start\", [0])\n",
        "        context_start_list = examples.get(\"context_start\", [0])\n",
        "        question_text = [q for q in examples.get(\"question\", [\"\"])]\n",
        "        start_positions = []\n",
        "        end_positions = []\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            question_text,\n",
        "            article_text,\n",
        "            truncation=\"only_second\",  # Truncate only the context\n",
        "            max_length=512,            # Limit input length\n",
        "            stride=128,                # Add a sliding window\n",
        "            return_overflowing_tokens=False,  # Handle long contexts\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "        # sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "        for i, offset in enumerate(offset_mapping):\n",
        "            answer = answer_text[i]\n",
        "            context = context_text[i]\n",
        "            article = article_text[i]\n",
        "            start_char = int(context_start_list[i]) + int(answer_start[i])\n",
        "            end_char = start_char + len(answer)\n",
        "\n",
        "\n",
        "            sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "            # Find the start and end of the context\n",
        "            idx = 0\n",
        "            while sequence_ids[idx] != 1:\n",
        "                idx += 1\n",
        "            context_start = idx\n",
        "            while sequence_ids[idx] == 1:\n",
        "                idx += 1\n",
        "            context_end = idx - 1\n",
        "\n",
        "            # If the answer is not fully inside the context, label is (0, 0)\n",
        "            if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "                start_positions.append(0)\n",
        "                end_positions.append(0)\n",
        "            else:\n",
        "                # Otherwise it's the start and end token positions\n",
        "                idx = context_start\n",
        "                while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                    idx += 1\n",
        "                start_positions.append(idx - 1)\n",
        "\n",
        "                idx = context_end\n",
        "                while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                    idx -= 1\n",
        "                end_positions.append(idx + 1)\n",
        "\n",
        "        inputs[\"start_positions\"] = start_positions\n",
        "        inputs[\"end_positions\"] = end_positions\n",
        "\n",
        "\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGbZ531XfzNW"
      },
      "source": [
        "# QA - BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDCqW6FbfzNW",
        "outputId": "e7f66e70-4719-4eb5-ac95-9cbc628b606d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.26.4\n",
            "[-1.4586302]\n"
          ]
        }
      ],
      "source": [
        "print(np.__version__)  # Check if NumPy is available\n",
        "print(torch.randn(1).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJwSpyvvp_JZ"
      },
      "outputs": [],
      "source": [
        "qa_bm25 = QA(\n",
        "    model_path=CURRENT_MODEL,\n",
        "    k = 100, overlap=0.0, num_chunks=1)\n",
        "wrong = qa_bm25.evaluate_retriever()\n",
        "metrics = qa_bm25.compute_retrieval_metrics()\n",
        "print(len(wrong))\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwp4ApHEfzNX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVStZCd5fzNX"
      },
      "outputs": [],
      "source": [
        "qa_bm25.run()\n",
        "qa_bm25.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk4_nRQlfzNX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXX0NTlEfzNX"
      },
      "source": [
        "# QA - FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvuQYobqfzNX",
        "outputId": "2f3d7e31-f68c-44df-b1b7-e8efd768dd3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating reader with model: /content/drive/My Drive/UP Files/IV - 2nd sem/CMSC 198.1/cebqa_roberta/new-split/xlmr_body-filtered/2025-04-04_05-13/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "qa_faiss = QA(\n",
        "    sample=500,\n",
        "    model_path=CURRENT_MODEL,\n",
        "    k = 10, overlap=0.0, num_chunks=1,\n",
        "    indexer_type=FAISS,\n",
        "    use_fine_tuned=False)\n",
        "wrong_faiss = qa_faiss.evaluate_retriever()\n",
        "metrics = qa_faiss.compute_retrieval_metrics()\n",
        "print(len(wrong_faiss))\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDx3-H0DfzNX"
      },
      "outputs": [],
      "source": [
        "qa_faiss.run()\n",
        "qa_faiss.evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}