{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtlagumbay/cebqa/blob/main/reader/cebqa_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CebQA Reader Component**\n",
        "Pretrained model: RoBERTa"
      ],
      "metadata": {
        "id": "otq_L0hL6e2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries**"
      ],
      "metadata": {
        "id": "fKdMhYwS7344"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_8cT2Gi-EAG",
        "outputId": "8c8dd88c-af9a-49c0-e512-0d34ce8b84bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from evaluate import load\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import XLMRobertaForQuestionAnswering, TrainingArguments, Trainer, XLMRobertaTokenizerFast, EarlyStoppingCallback, pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
        "from huggingface_hub import login\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "from IPython.display import display\n",
        "from sklearn.metrics import f1_score\n",
        "import re\n",
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VGIEDXaL73ZW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "q2596jflKFZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f76ed8-b6d4-456b-bb4e-9467bfafd0e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Constants**"
      ],
      "metadata": {
        "id": "3quPalaN_3Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CEBQA_DATASET = \"jhoannarica/cebquad\"\n",
        "DRIVE_ROOT = \"/content/drive/Shareddrives/cebqa_roberta/xlmr\"\n",
        "OUTPUT_DIRECTORY = \"training_output\"\n",
        "LOGS_DIRECTORY = \"LOGS\"\n",
        "MODEL_DIRECTORY = \"model\"\n",
        "TOKENIZER_DIRECTORY = \"tokenizer\""
      ],
      "metadata": {
        "id": "VuU1OcfI_5UJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utils**"
      ],
      "metadata": {
        "id": "4yzn4D2wKyWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def timestamp(append):\n",
        "  return datetime.datetime.now().strftime(\"%Y-%m-%d_%H\")+\"-\"+str(append)\n",
        "\n",
        "def get_output_directory(batch_timestamp):\n",
        "  return f\"{DRIVE_ROOT}/{batch_timestamp}/{OUTPUT_DIRECTORY}\"\n",
        "\n",
        "def get_logs_directory(batch_timestamp):\n",
        "  return f\"{DRIVE_ROOT}/{batch_timestamp}/{LOGS_DIRECTORY}\"\n",
        "\n",
        "def get_model_directory(batch_timestamp):\n",
        "  return f\"{DRIVE_ROOT}/{batch_timestamp}/{MODEL_DIRECTORY}\"\n",
        "\n",
        "def get_tokenizer_directory(batch_timestamp):\n",
        "  return f\"{DRIVE_ROOT}/{batch_timestamp}/{TOKENIZER_DIRECTORY}\""
      ],
      "metadata": {
        "id": "RT6PmYJ3K0W1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Dataset**"
      ],
      "metadata": {
        "id": "a7eEcbNh6w1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access dataset"
      ],
      "metadata": {
        "id": "ICMgS9hi_X9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p1_d7Fdx4Gmd"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(CEBQA_DATASET)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][120]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT27aUa3KMlG",
        "outputId": "63068e40-9347-43e9-92b2-9f5515549097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '01529-002',\n",
              " 'article_id': 1529,\n",
              " 'article_title': 'Tourist van nahulog kay driver nakatulog',\n",
              " 'article_body': 'Nangalandig sa emergency room sa Badian District Hospital sa Brgy. Poblacion, Badian, habagatang Sugbo, ang upat ka mga turista ug drayber sa van nga ilang gisakyan human naaksidente sa alas 3:40 sa kaadlawon sa Biyernes, Nobiyembre 17, 2023, sa Brgy. Poblacion. Ang drayber nakatulog kay hayan lapoy pa kini sa iyang kapin sa 100 ka kilometro nga biyahe sa amihanang Sugbo. Hinuon minor injuries lang ang naangkon sa mga biktima busa nakagawas ra dayon sa ospital human matambali ug mahiling. Basi sa nakuhang kasayuran sa Superbalita sa Cebu gikan sa kasaligang tinubdan, nailhan ang mga biktima nga turista nga puro taga San Antonio, Tondo, Manila, nga sila si Antonietto Avila Libunao, 64, minyo; iyang asawa nga si Carmen Pacione; Lorence Pacis Paclibon , 40, minyo; ug anak niini nga si Pacomios Pacis Paclibon, 5. Samtang ang drayber nga naangol giila nga si Emeniano Jorge Pacto Pacuan, 24, ulitawo, taga Lamintak Sur, Medellin, amihanang Sugbo. Matod sa tinubdan nga si Pacuan gikan naghatod sa iyang laing guests sa norte sa Sugbo. Pagkahuman, sa mga ala 2 sa kaadlawon sa Nobiyembre 17, niadto siya sa Mactan International Airport aron pagkuha sa mao nga mga turista. Ang grupo padung sa lungsod sa Samboan, Sugbo, apan pag-abot sa taytayan sa Poblacion, Badian nakatagpilaw si Pacuan ug nahasimang ang sakyanan ug nahulog kilid sa bridge. Wa kini malahos sa sapa kay nasangit sa kanipaan. Dali sila nga gipangdala sa tambalanan sa emergency responders. Paglabay sa pipila ka mga oras, nakagawas ra silang tanan sa tambalanan.',\n",
              " 'question': 'Unsa ang kahimtang sa mga biktima human sa aksidente?',\n",
              " 'context': {'end': 493,\n",
              "  'start': 375,\n",
              "  'text': 'Hinuon minor injuries lang ang naangkon sa mga biktima busa nakagawas ra dayon sa ospital human matambali ug mahiling.'},\n",
              " 'answer': {'end': 26, 'start': 7, 'text': 'minor injuries lang'},\n",
              " 'context_start': 375,\n",
              " 'context_end': 493,\n",
              " 'answer_start': 7,\n",
              " 'answer_end': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to track the longest article\n",
        "longest_article = None\n",
        "max_length = 0\n",
        "\n",
        "# Iterate through each article in the train dataset\n",
        "for article in dataset[\"train\"]:\n",
        "    # Concatenate article_body and context\n",
        "    combined_text = article[\"article_body\"] + article[\"question\"]\n",
        "\n",
        "    # Calculate the length of the combined text\n",
        "    combined_length = len(combined_text)\n",
        "\n",
        "    # Update if this article is the longest found so far\n",
        "    if combined_length > max_length:\n",
        "        max_length = combined_length\n",
        "        longest_article = article\n",
        "\n",
        "# Print the longest article and its length\n",
        "print(f\"Longest combined article length: {max_length}\")\n",
        "print(f\"Longest article: {longest_article}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4pe3xzNR5hn",
        "outputId": "011133b4-d7ef-49fb-fe1d-88d31b5ad933"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest combined article length: 5911\n",
            "Longest article: {'id': '00127-003', 'article_id': 127, 'article_title': 'Senado tensiyonado atol sa pag-imbestigar ni Balderas', 'article_body': 'Puno sa tensyon atol sa imbestigasyon sa Senado sa gi-raid nga ilegal nga Pogo hub niadtong Lunes, Septiyembre 9, 2024, tungod kay ang mga magbabalaod nangasuko ni dismissed Bamban, Tarlac Mayor Aretha Balderas, kinsa nagdumili sa pagtubag sa ilang mga pangutana. Samtang ang mga magbabalaod nangutana kung giunsa niya ug ang pipila ka mga miyembro sa iyang pamilya mibiya sa nasod, si Balderas nagdumili sa paghingalan sa tawo nga nagpahigayon sa ilang pag-ikyas, tungod sa kahadlok sa iyang kinabuhi. Gisuwat hinuon niya ang ngalan sa tawo sa usa ka papel sa hangyo ni Senate President Pro Tempore Stanford Baldomar. Gihangyo ni Balderas ang mga magbabalaod nga dili isulti og kusog ang ngalan. \"Do not tell the senators what to do with the information. Pinagbibigyan ka namin isulat sa papel,\" matod ni Senate Committee on Women, Children, Family Relations, and Gender Equality chair Senator Charina Alcaide. \"Nauubos na ‘yung pasensya namin,\" matod ni Alcaide. KUSTODIYA SA PNPGipahinumdoman ni Alcaide si Balderas nga luwas na siya sa kustodiya sa Philippine National Police (PNP). Si Balderas niingon nga andam siya nga ilhon ang mga ngalan sa mga tawo nga mitabang kaniya apan dili sa publiko alang sa katuyoan sa kaluwasan. \"Gustong-gusto mo pala (sabihin), e di sabihin mo. Nakakapikon ka na. Nakakagigil ka na,\" matod ni Baldomar. Si Balderas niingon nga ang tawo nga nitabang nila sa pagbiya mao usab ang naghimo nga sila makaeskapo. \"Siya nag-initiate. . . Nu’ng una po siya ang nagdesisyon para sa akin,\" matod ni Balderas. \"Actually, hindi po tulong ang hiningi ko sa kanya. Actually, to be exact, medyo pinagsalitaan ko po siya nang hindi maganda,\" dason niya. Si Baldomar nagpadayon nga nakasawsay kang Balderas, nga nag-ingon nga “kabuang” ug “katingad-an” nga dili niya ipahibalo ang ngalan sa tawo nga mitabang kanila. \"Pinagsalitaan mo siya nang masama pero siya ang nag-facilitate ng pagtakas ninyo sa Pilipinas. . . Siyempre ‘pag ako pinagsalitaan mo nang masama, hindi na kita tutulungan,\" matod ni Baldomar. \"Bakit ‘di mo pwedeng i-divulge sa publiko? May sikreto ba kayo? This is just a simple question. Pinagsalitaan mo nang masama, absurd, para sa akin it\\'s weird, tapos siya pa ang tumulong sa iyo na makatakas sa Pilipinas,\" dugang niya. Si Alcaide niingon nga nitumaw na sa nangaging mga hearing ang ngalan sa tawo kinsa, sumala ni Balderas, nitabang kanila. Matod ni Baldomar nga anaa karon sa Taiwan ang giila ni Balderas. Siya usa ka naghupot sa lima ka mga pasaporte. Si Balderas miingon nga wala siya kahibalo sa maong impormasyon. Si Balderas usab miingon nga sila, ang iyang gituohang mga igsuon nga sila si Lebron ug Aching, mibiya sa Pilipinas sakay sa barko, usa ka gabii, niadtong Hulyo. Siya niingon nga misakay sila og yate gikan sa pantalan sa Metro Manila dayon mibalhin sa mas dakong barko sa tungatunga sa kadagatan. Si Balderas miingon nga nagpabilin sila sa lawak sa mas dakong sakayan sulod sa pipila ka adlaw. Wala sila gitugotan sa paggawas o bisan sa pagsusi sa ilang mga cell phone. \"Madilim pa rin po. . . Basta dagat po,\" matod ni Balderas. \"Siguro mga four or three or five (days). Basta matagal po. . . Four days po siguro. . . Hindi po kami pinapalabas. Kung puwede lang umatras, aatras na po ako. . . Nakakatakot po talaga. ¦ Gusto ko na po umuwi. Gusto ko na po bumalik,\" dason niya. Giangkon ni Balderas nga nasayop siya kay gusto siyang moeskapo. NASAYOPSiya miingon nga gikan sa dako nga sakayan, sila mibalhin pag-usab ngadto sa usa ka mas gamay nga sakayan, nga nagdala kanila ngadto sa Malaysia, nga wala mahibalo kon asa sila sa tukma tungod kay wa sila tugoti sa pagtan-aw sa palibot. Ang mga pahayag ni Balderas nagpamatuod sa naunang pagsaysay ni Aching kung giunsa nila pag-ikyas. \"Walang tumulong po ni isang Filipino or Filipina. . . Immigration, wala po. Government officials, wala rin po. Filipino, wala po. Wala pong tumulong,\" matod niya. \"That\\'s impossible. Imposibleng walang tumulong sayo upang makatakas dito sa Pilipinas,\" tubag ni Baldomar. Gipanghimakak usab ni Balderas ang pagbayad sa mga nagpahigayon sa ilang pag-ikyas og P200 milyunes. Gitataw usab niya nga usa siya ka Pinay human siya gihangyo sa pagkompirmar sa fingerprint examination nga gihimo sa National Bureau of Investigation (NBI), nga nagmando nga siya usa ug parehas sa Chinese national nga si Balderas Baldonado Balenti. Gipadayon niya nga ang iyang amahan mao si Balderas Balequia Balestramon ug gilimod nga ang iyang inahan mao si Balgos Balidiong Yi. \"Honestly, hindi ko po alam kung paano nangyari. Basta alam ko ako po si Aretha Balderas. At pasensya na rin po kung hindi kayo naniniwala. ¦Lumaki po ako na alam ko Filipino ako,\" pasabot niya. \"Ang alam ko po (All I know is) I was born in Tarlac,\" dugang niya. Gipasanginlan ni Senador Eulogia Alcano si Balderas nga padayong namakak sa ilang mga nawong. Nagdumili usab si Balderas sa pagtubag sa ubang mga pangutana tungod kay gihangyo niya ang iyang katungod sa pagpasangil sa kaugalingon. Siya misaad, bisan pa, sa pagtubag niini nga mga alegasyon sa atubangan sa tukma nga korte. GI-CONTEMPTSa ikaduhang higayon, si Balderas cited in contempt sa Senado. \"This is a blatant defiance of the legislative’s constitutional power of inquiry. Lumalabas na pinaglalaruan mo ang aming batas at pinapaikot mo ang mga Pilipino, pero ibahin mo ang Senado,\" said Alcaide. Human sa pagdungog, si Balderas gibalik sa PNP Custodial Facility diin siya gitanggong alang sa kasong graft and corruption. Si Balderas gidakop sa Tangerang, Indonesia niadtong Septiyembre 3 pinasikad sa arrest order nga giluwatan sa Senate panel kalabot sa nagpadayong imbestigasyon sa gironda nga ilegal nga Pogo hub sa Bamban, Tarlac. Katapusang higayon nga nitambong siya sa imbestigasyon sa Senado niadtong Mayo. / TPM / SunStar Philippines', 'question': 'Unsa ang hangyo ni Balderas sa mga magbabalaod bahin sa ngalan sa tawo nga nagtabang kaniya?', 'context': {'end': 696, 'start': 619, 'text': 'Gihangyo ni Balderas ang mga magbabalaod nga dili isulti og kusog ang ngalan.'}, 'answer': {'end': 76, 'start': 45, 'text': 'dili isulti og kusog ang ngalan'}, 'context_start': 619, 'context_end': 696, 'answer_start': 45, 'answer_end': 76}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Dataset**"
      ],
      "metadata": {
        "id": "3-r_lcGs60xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare tokenizer"
      ],
      "metadata": {
        "id": "D-9YTrAlS6sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")"
      ],
      "metadata": {
        "id": "lGZQjrRBS8d5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.model_max_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUBZf_XbUih3",
        "outputId": "e52a42e9-a150-4b2e-be60-f46b673c63f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "hqG6RvzTS-mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_incomplete_examples(example):\n",
        "    # Ensure both \"question\" and \"context\" exist and are non-empty\n",
        "    return \"question\" in example and example[\"question\"] and \\\n",
        "           \"context\" in example and \"text\" in example[\"context\"] and \\\n",
        "           example[\"context\"][\"text\"] and example[\"answer\"][\"text\"]\n"
      ],
      "metadata": {
        "id": "RJmFpqXfihcH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    context_text = [context.get(\"text\", \"\") for context in examples.get(\"context\", [{}])]\n",
        "    question_text = examples.get(\"question\", [\"\"])\n",
        "\n",
        "    tokenized_examples = tokenizer(\n",
        "        question_text,\n",
        "        context_text,\n",
        "        truncation=\"only_second\",  # Truncate only the context\n",
        "        max_length=512,            # Limit input length\n",
        "        stride=128,                # Add a sliding window\n",
        "        return_overflowing_tokens=True,  # Handle long contexts\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
        "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "    # Lists to store calculated start and end positions\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        start_token = 0\n",
        "        end_token = 0\n",
        "        sample_index = sample_mapping[i]\n",
        "        answer = examples[\"answer\"][sample_index]\n",
        "\n",
        "        # Handle missing or empty answers\n",
        "        if len(answer[\"text\"]) == 0:\n",
        "            start_positions.append(start_token)\n",
        "            end_positions.append(end_token)\n",
        "            continue\n",
        "\n",
        "        # Get the answer's start and end character positions\n",
        "        start_char = answer[\"start\"]\n",
        "        end_char = answer[\"end\"]\n",
        "\n",
        "        # Get the sequence IDs to identify the context part\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # Identify the start and end of the context\n",
        "        context_start = sequence_ids.index(1)\n",
        "        context_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
        "\n",
        "        # Check if the answer is out of the bounds of the context\n",
        "        if start_char < offsets[context_start][0] or end_char > offsets[context_end][1]:\n",
        "            start_positions.append(start_token)\n",
        "            end_positions.append(end_token)\n",
        "            continue\n",
        "\n",
        "        # Find start and end tokens for the answer\n",
        "        start_token = next(\n",
        "            (idx for idx, offset in enumerate(offsets)\n",
        "            if offset[0] <= start_char <= offset[1]),\n",
        "            None\n",
        "        )\n",
        "        end_token = next(\n",
        "            (idx for idx, offset in enumerate(offsets)\n",
        "            if offset[0] <= end_char <= offset[1]),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        if start_token is None:\n",
        "            raise ValueError(\"Start character position not found in token offsets.\")\n",
        "\n",
        "        if end_token is None:\n",
        "            raise ValueError(\"Start character position not found in token offsets.\")\n",
        "\n",
        "        start_positions.append(start_token)\n",
        "        end_positions.append(end_token)\n",
        "\n",
        "    # Add start and end positions to the tokenized examples\n",
        "    tokenized_examples[\"start_positions\"] = start_positions\n",
        "    tokenized_examples[\"end_positions\"] = end_positions\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_dataset = dataset.filter(filter_incomplete_examples).map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "EDWQ_moz64VR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset[\"train\"].features"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKdD_1Y0XTVH",
        "outputId": "227f280c-5654-45ad-eb49-8cf1e975b509"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': Value(dtype='string', id=None),\n",
              " 'article_id': Value(dtype='int64', id=None),\n",
              " 'article_title': Value(dtype='string', id=None),\n",
              " 'article_body': Value(dtype='string', id=None),\n",
              " 'question': Value(dtype='string', id=None),\n",
              " 'context': {'end': Value(dtype='int64', id=None),\n",
              "  'start': Value(dtype='int64', id=None),\n",
              "  'text': Value(dtype='string', id=None)},\n",
              " 'answer': {'end': Value(dtype='int64', id=None),\n",
              "  'start': Value(dtype='int64', id=None),\n",
              "  'text': Value(dtype='string', id=None)},\n",
              " 'context_start': Value(dtype='int64', id=None),\n",
              " 'context_end': Value(dtype='int64', id=None),\n",
              " 'answer_start': Value(dtype='int64', id=None),\n",
              " 'answer_end': Value(dtype='int64', id=None),\n",
              " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
              " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
              " 'offset_mapping': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
              " 'overflow_to_sample_mapping': Value(dtype='int64', id=None),\n",
              " 'start_positions': Value(dtype='int64', id=None),\n",
              " 'end_positions': Value(dtype='int64', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Splitting"
      ],
      "metadata": {
        "id": "c5NOLjCS7wYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset[\"train\"]\n",
        "val_dataset = tokenized_dataset[\"validation\"]\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "gqZN7rgM705O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5dacace-e84a-4864-e1ff-779ccee88170"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'article_id', 'article_title', 'article_body', 'question', 'context', 'answer', 'context_start', 'context_end', 'answer_start', 'answer_end', 'input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
              "    num_rows: 19340\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "k_fiMlMM8SMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-Trained RoBERTa"
      ],
      "metadata": {
        "id": "uSTMRkFf8GgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = XLMRobertaForQuestionAnswering.from_pretrained(\"xlm-roberta-base\")\n"
      ],
      "metadata": {
        "id": "6NttjYb58RG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48032e53-4c12-435d-cb09-e26a2a100c17"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ],
      "metadata": {
        "id": "Y4RupQI5LpfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping parameters\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,  # Number of evaluations with no improvement before stopping\n",
        "    early_stopping_threshold=0.0  # Minimum change in the metric to qualify as an improvement\n",
        ")"
      ],
      "metadata": {
        "id": "ANAgY7XdLrSc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Argument"
      ],
      "metadata": {
        "id": "aWmbCFT-8azK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to normalize text for comparison\n",
        "def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove punctuation, articles, and extra whitespace.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Function to compute F1 score\n",
        "def compute_f1(pred, truth):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    # Calculate common tokens\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    # Precision and Recall\n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    # F1 score\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "# Function to compute Exact Match (EM)\n",
        "def compute_exact_match(pred, truth):\n",
        "    return int(normalize_text(pred) == normalize_text(truth))\n",
        "\n",
        "# Function to compute Sentence Match\n",
        "def compute_sentence_match(pred, truth):\n",
        "    pred_normalized = normalize_text(pred)\n",
        "    truth_normalized = normalize_text(truth)\n",
        "    return int(pred_normalized in truth_normalized or truth_normalized in pred_normalized)"
      ],
      "metadata": {
        "id": "da3dA89fe5od"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_qa_predictions(examples, start_logits, end_logits, tokenizer):\n",
        "    \"\"\"\n",
        "    Convert model logits into readable answers.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(start_logits)):\n",
        "        start_idx = np.argmax(start_logits[i])  # Best start index\n",
        "        end_idx = np.argmax(end_logits[i])  # Best end index\n",
        "\n",
        "        # Ensure valid span\n",
        "        if start_idx >= len(examples[\"input_ids\"][i]) or end_idx >= len(examples[\"input_ids\"][i]):\n",
        "            predictions.append(\"\")\n",
        "            continue\n",
        "\n",
        "        if start_idx > end_idx:  # If invalid prediction\n",
        "            predictions.append(\"\")\n",
        "            continue\n",
        "\n",
        "        # Decode the predicted answer\n",
        "        input_ids = examples[\"input_ids\"][i]\n",
        "        answer_tokens = input_ids[start_idx : end_idx + 1]\n",
        "        prediction = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "StAVyBSG2aEC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load(\"squad\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    start_logits, end_logits = logits\n",
        "    predictions = postprocess_qa_predictions(val_dataset, start_logits, end_logits, tokenizer)\n",
        "\n",
        "    # Format references correctly\n",
        "    references = [\n",
        "        {\"id\": str(i), \"answers\": {\"text\": [data[\"answer\"][\"text\"]], \"answer_start\": [data[\"answer\"][\"start\"]]}}\n",
        "        for i, data in enumerate(val_dataset)\n",
        "    ]\n",
        "\n",
        "    # Compute F1\n",
        "    return metric.compute(predictions=[{\"id\": str(i), \"prediction_text\": pred} for i, pred in enumerate(predictions)], references=references)\n",
        "\n"
      ],
      "metadata": {
        "id": "kUAjFHGhtSbM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset[0]"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loMtSNGwMH7x",
        "outputId": "61fa01e3-675c-4c83-af15-b4b35c99f46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '01543-003',\n",
              " 'article_id': 1543,\n",
              " 'article_title': 'Japanese music video, nakig-collab sa Pinoy artists, midaog na sab',\n",
              " 'article_body': 'ANG siyudad sa Hadano, Kanagawa Prefecture, Japan mipahigayon sa ilang 2nd Hadastragram movie contest niadtong Nobiyembre 6, 2023. Moabot ngadto sa 195 ka entry nga gisalmot diin ang \"Bathroom Orchestra Instrumental\" music video sa Japanese musician-film maker Jonneper Padil, a. k. a. iwapt, ang gideklarar nga grand prix champion. Ang maong music video adunay collaboration sa Filipino musician nga naglakip nilang Erlinda Leones ug Randy Lepasana, drummer sa OPM band nga Neocolours. Ang maong awit nga gi-compose ni iwapt gi-record sa Manila niadtong 2010. Ang iyang music video nag-promote sa Hadano City nga nagpakita sa inila nga underground spring water sa maong dapit. Gi-showcase sab ang ilang nature-rich parks ug observatories, sikat nila nga delicacies, ug ang dapit sa Hadano diin gipahigayon ang 2020 Tokyo Olympics.',\n",
              " 'question': 'Kinsa ang nagdaog nga music video sa Hadastragram movie contest?',\n",
              " 'context': {'end': 333,\n",
              "  'start': 0,\n",
              "  'text': 'ANG siyudad sa Hadano, Kanagawa Prefecture, Japan mipahigayon sa ilang 2nd Hadastragram movie contest niadtong Nobiyembre 6, 2023. Moabot ngadto sa 195 ka entry nga gisalmot diin ang \"Bathroom Orchestra Instrumental\" music video sa Japanese musician-film maker Jonneper Padil, a. k. a. iwapt, ang gideklarar nga grand prix champion. '},\n",
              " 'answer': {'end': 275, 'start': 261, 'text': 'Jonneper Padil'},\n",
              " 'context_start': 0,\n",
              " 'context_end': 333,\n",
              " 'answer_start': 261,\n",
              " 'answer_end': 275,\n",
              " 'input_ids': [0,\n",
              "  37029,\n",
              "  433,\n",
              "  348,\n",
              "  4197,\n",
              "  85,\n",
              "  1663,\n",
              "  817,\n",
              "  19612,\n",
              "  1202,\n",
              "  57,\n",
              "  185693,\n",
              "  2816,\n",
              "  25561,\n",
              "  14277,\n",
              "  62757,\n",
              "  32,\n",
              "  2,\n",
              "  2,\n",
              "  49754,\n",
              "  78,\n",
              "  9076,\n",
              "  12409,\n",
              "  57,\n",
              "  185693,\n",
              "  157,\n",
              "  4,\n",
              "  59510,\n",
              "  21256,\n",
              "  1914,\n",
              "  17928,\n",
              "  6644,\n",
              "  4,\n",
              "  15758,\n",
              "  324,\n",
              "  61118,\n",
              "  1758,\n",
              "  9480,\n",
              "  57,\n",
              "  41455,\n",
              "  116,\n",
              "  2208,\n",
              "  185693,\n",
              "  2816,\n",
              "  25561,\n",
              "  14277,\n",
              "  62757,\n",
              "  300,\n",
              "  712,\n",
              "  18176,\n",
              "  438,\n",
              "  40165,\n",
              "  36300,\n",
              "  305,\n",
              "  4,\n",
              "  140429,\n",
              "  5,\n",
              "  2501,\n",
              "  94445,\n",
              "  817,\n",
              "  71,\n",
              "  188,\n",
              "  57,\n",
              "  91809,\n",
              "  156,\n",
              "  42805,\n",
              "  817,\n",
              "  3016,\n",
              "  2317,\n",
              "  18397,\n",
              "  45,\n",
              "  73,\n",
              "  348,\n",
              "  44,\n",
              "  8023,\n",
              "  42294,\n",
              "  306,\n",
              "  198624,\n",
              "  73750,\n",
              "  289,\n",
              "  58,\n",
              "  19612,\n",
              "  1202,\n",
              "  57,\n",
              "  148926,\n",
              "  19612,\n",
              "  3378,\n",
              "  9,\n",
              "  8551,\n",
              "  6,\n",
              "  55474,\n",
              "  21240,\n",
              "  86,\n",
              "  1264,\n",
              "  22017,\n",
              "  379,\n",
              "  4,\n",
              "  10,\n",
              "  5,\n",
              "  472,\n",
              "  5,\n",
              "  10,\n",
              "  5,\n",
              "  17,\n",
              "  634,\n",
              "  6328,\n",
              "  4,\n",
              "  348,\n",
              "  38630,\n",
              "  343,\n",
              "  320,\n",
              "  147,\n",
              "  817,\n",
              "  9963,\n",
              "  22135,\n",
              "  102995,\n",
              "  5,\n",
              "  2,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'offset_mapping': [[0, 0],\n",
              "  [0, 3],\n",
              "  [3, 5],\n",
              "  [6, 9],\n",
              "  [10, 13],\n",
              "  [13, 15],\n",
              "  [15, 17],\n",
              "  [18, 21],\n",
              "  [22, 27],\n",
              "  [28, 33],\n",
              "  [34, 36],\n",
              "  [37, 41],\n",
              "  [41, 45],\n",
              "  [45, 49],\n",
              "  [50, 55],\n",
              "  [56, 63],\n",
              "  [63, 64],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 3],\n",
              "  [4, 6],\n",
              "  [6, 8],\n",
              "  [8, 11],\n",
              "  [12, 14],\n",
              "  [15, 19],\n",
              "  [19, 21],\n",
              "  [21, 22],\n",
              "  [23, 27],\n",
              "  [27, 31],\n",
              "  [32, 35],\n",
              "  [35, 38],\n",
              "  [38, 42],\n",
              "  [42, 43],\n",
              "  [44, 49],\n",
              "  [50, 52],\n",
              "  [52, 55],\n",
              "  [55, 58],\n",
              "  [58, 61],\n",
              "  [62, 64],\n",
              "  [65, 70],\n",
              "  [71, 72],\n",
              "  [72, 74],\n",
              "  [75, 79],\n",
              "  [79, 83],\n",
              "  [83, 87],\n",
              "  [88, 93],\n",
              "  [94, 101],\n",
              "  [102, 104],\n",
              "  [104, 106],\n",
              "  [106, 110],\n",
              "  [111, 113],\n",
              "  [113, 116],\n",
              "  [116, 121],\n",
              "  [122, 123],\n",
              "  [123, 124],\n",
              "  [125, 129],\n",
              "  [129, 130],\n",
              "  [131, 133],\n",
              "  [133, 137],\n",
              "  [138, 141],\n",
              "  [141, 142],\n",
              "  [142, 144],\n",
              "  [145, 147],\n",
              "  [148, 151],\n",
              "  [152, 154],\n",
              "  [155, 160],\n",
              "  [161, 164],\n",
              "  [165, 167],\n",
              "  [167, 170],\n",
              "  [170, 173],\n",
              "  [174, 176],\n",
              "  [176, 178],\n",
              "  [179, 182],\n",
              "  [183, 184],\n",
              "  [184, 186],\n",
              "  [186, 190],\n",
              "  [190, 192],\n",
              "  [193, 202],\n",
              "  [203, 213],\n",
              "  [213, 215],\n",
              "  [215, 216],\n",
              "  [217, 222],\n",
              "  [223, 228],\n",
              "  [229, 231],\n",
              "  [232, 240],\n",
              "  [241, 246],\n",
              "  [246, 249],\n",
              "  [249, 250],\n",
              "  [250, 254],\n",
              "  [255, 256],\n",
              "  [255, 260],\n",
              "  [261, 264],\n",
              "  [264, 266],\n",
              "  [266, 269],\n",
              "  [270, 273],\n",
              "  [273, 275],\n",
              "  [275, 276],\n",
              "  [277, 278],\n",
              "  [278, 279],\n",
              "  [280, 281],\n",
              "  [281, 282],\n",
              "  [283, 284],\n",
              "  [284, 285],\n",
              "  [286, 287],\n",
              "  [287, 289],\n",
              "  [289, 291],\n",
              "  [291, 292],\n",
              "  [293, 296],\n",
              "  [297, 300],\n",
              "  [300, 302],\n",
              "  [302, 305],\n",
              "  [305, 307],\n",
              "  [308, 311],\n",
              "  [312, 317],\n",
              "  [318, 322],\n",
              "  [323, 331],\n",
              "  [331, 332],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0],\n",
              "  [0, 0]],\n",
              " 'overflow_to_sample_mapping': 0,\n",
              " 'start_positions': 91,\n",
              " 'end_positions': 95}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_xlmr():\n",
        "    batch_timestamp = timestamp(0)\n",
        "    # Suggest values for hyperparameters\n",
        "    # learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n",
        "    # batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    # num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
        "    # weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-3, 0.1)\n",
        "\n",
        "    learning_rate = 2e-5\n",
        "    batch_size = 16\n",
        "    num_train_epochs = 3\n",
        "    weight_decay = 1e-3\n",
        "\n",
        "    # Define training arguments with suggested values\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=get_output_directory(batch_timestamp),\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        logging_dir=get_logs_directory(batch_timestamp),\n",
        "        logging_steps=10,\n",
        "        save_total_limit=3,\n",
        "        bf16=True  # Best for A100\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        # compute_metrics=compute_metrics,\n",
        "        callbacks=[early_stopping_callback]\n",
        "    )\n",
        "\n",
        "     # Train and evaluate the model\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(eval_results)\n",
        "\n",
        "    model.save_pretrained(get_model_directory(batch_timestamp))\n",
        "    tokenizer.save_pretrained(get_output_directory(batch_timestamp))\n",
        "\n",
        "    return eval_results"
      ],
      "metadata": {
        "id": "reOEvEudn3e1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(finetune_xlmr, n_trials=1)\n",
        "\n",
        "# Get the best trial\n",
        "best_trial = study.best_trial\n",
        "# Print best trial number and its hyperparameters\n",
        "print(f\"Best Trial: {best_trial.number}\")\n",
        "print(\"Best Hyperparameters:\", best_trial.params)\n",
        "print(f\"Best F1 Score: {best_trial.value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "bxfPiSNSp001",
        "outputId": "ea14768e-1658-4e91-9111-83636dde8376"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-10 13:44:01,276] A new study created in memory with name: no-name-525be363-eb88-442a-80fd-1fb72ab389ac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10_13-44-0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-46-ac82ea0702a3>:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n",
            "<ipython-input-46-ac82ea0702a3>:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-3, 0.1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1210' max='4836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1209/4836 01:50 < 05:31, 10.96 it/s, Epoch 1.00/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='173' max='173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [173/173 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_xlmr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "kJs6MG-AaPmJ",
        "outputId": "9ad04583-4118-4cea-9cb5-35d83a13e2f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjtlagumbay\u001b[0m (\u001b[33mjtlagumbay-university-of-the-philippines\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250310_145335-6abvu13a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jtlagumbay-university-of-the-philippines/huggingface/runs/6abvu13a' target=\"_blank\">/content/drive/Shareddrives/cebqa_roberta/xlmr/2025-03-10_14-0/training_output</a></strong> to <a href='https://wandb.ai/jtlagumbay-university-of-the-philippines/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jtlagumbay-university-of-the-philippines/huggingface' target=\"_blank\">https://wandb.ai/jtlagumbay-university-of-the-philippines/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jtlagumbay-university-of-the-philippines/huggingface/runs/6abvu13a' target=\"_blank\">https://wandb.ai/jtlagumbay-university-of-the-philippines/huggingface/runs/6abvu13a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3627' max='3627' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3627/3627 06:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.159500</td>\n",
              "      <td>2.241912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.034800</td>\n",
              "      <td>1.794427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.668300</td>\n",
              "      <td>1.718784</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='173' max='173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [173/173 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.7187840938568115, 'eval_runtime': 4.9326, 'eval_samples_per_second': 559.947, 'eval_steps_per_second': 35.073, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.7187840938568115,\n",
              " 'eval_runtime': 4.9326,\n",
              " 'eval_samples_per_second': 559.947,\n",
              " 'eval_steps_per_second': 35.073,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating the model**"
      ],
      "metadata": {
        "id": "85j__wkg9Mny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating"
      ],
      "metadata": {
        "id": "KPv14zFU9Pye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "HryRtAmv9T8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  model\n",
        "  tokenizer\n",
        "except:\n",
        "  print(\"none\")\n",
        "    # model_path = \"/content/drive/Shareddrives/cebqa_roberta/xlmr/2025-02-25_01/model\"\n",
        "    # tokenizer_path = \"/content/drive/Shareddrives/cebqa_roberta/xlmr/2025-02-25_01/tokenizer\"\n",
        "\n",
        "    # model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "36M64RE79TuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c20732-3cb0-4b2c-8fed-ab6d19cba315"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Dataset.from_dict({\n",
        "    \"question\": [sample[\"question\"] for sample in dataset[\"train\"]],\n",
        "    \"context\": [sample[\"article_body\"] for sample in dataset[\"train\"]]\n",
        "})\n",
        "model_outputs = qa_pipeline(test_dataset)\n",
        "\n",
        "results_list = []\n",
        "for sample, model_output in zip(dataset[\"train\"], model_outputs):\n",
        "  # print(sample)\n",
        "    expected_answer = sample[\"answer\"][\"text\"] if sample[\"answer\"][\"text\"] else \"N/A\"  # Handle empty answers\n",
        "    actual_answer = model_output[\"answer\"]\n",
        "\n",
        "    results_list.append({\n",
        "        \"Question\": sample[\"question\"],\n",
        "        \"Expected Answer\": expected_answer,\n",
        "        \"Actual Answer\": actual_answer\n",
        "    })\n",
        "\n",
        "for result in results_list:\n",
        "    expected = result[\"Expected Answer\"]\n",
        "    actual = result[\"Actual Answer\"]\n",
        "\n",
        "    result[\"F1 Score\"] = compute_f1(actual, expected)\n",
        "    result[\"Exact Match\"] = compute_exact_match(actual, expected)\n",
        "    result[\"Sentence Match\"] = compute_sentence_match(actual, expected)\n",
        "\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "df = pd.DataFrame(results_list)\n",
        "\n",
        "# Display as a table\n",
        "display(df)\n"
      ],
      "metadata": {
        "id": "4dstIijiNWjH",
        "outputId": "eae57a56-7af9-4587-958e-0b57b2f94a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'answer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-465be2cf6a2d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresults_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mexpected_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"N/A\"\u001b[0m  \u001b[0;31m# Handle empty answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mactual_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'answer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_f1 = df[\"F1 Score\"].mean()\n",
        "avg_em = df[\"Exact Match\"].mean()\n",
        "avg_sm = df[\"Sentence Match\"].mean()\n",
        "\n",
        "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
        "print(f\"Average Exact Match: {avg_em:.4f}\")\n",
        "print(f\"Average Sentence Match: {avg_sm:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CfBTk0RgPVH",
        "outputId": "62b14034-cb6b-4cf2-e233-27d9ffbbbbe8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 Score: 0.4491\n",
            "Average Exact Match: 0.3123\n",
            "Average Sentence Match: 0.4728\n"
          ]
        }
      ]
    }
  ]
}